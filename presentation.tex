% no notes
%\documentclass{beamer}
% notes and slides
\documentclass[notes]{beamer}
% notes only
% \documentclass[notes=only]{beamer}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multimedia}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{url}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\usepackage{standalone}
\usepackage{adjustbox}
\usepackage{lmodern}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{standalone}
\usepackage{csquotes}


\PassOptionsToPackage{american}{babel} % change this to your language(s), main language last
% Spanish languages need extra options in order to work with this template
% \PassOptionsToPackage{spanish,es-lcroman}{babel}
\usepackage{babel}

\PassOptionsToPackage{%
  backend=biber,bibencoding=utf8, %instead of bibtex
  %backend=bibtex8,bibencoding=ascii,%
  language=auto,%
  style=numeric-comp,%
  %style=authoryear-comp, % Author 1999, 2010
  %bibstyle=authoryear,dashed=false, % dashed: substitute rep. author with ---
  style=alphabetic,
  sorting=nyt, % name, year, title
  maxbibnames=10, % default: 3, et al.
  %backref=true,%
  %natbib=true % natbib compatibility mode (\citep and \citet still work)
}{biblatex}
\usepackage{biblatex}

\addbibresource{bib.bib}

\usetheme{metropolis}           % Use metropolis theme
\setbeamertemplate{caption}[default]
\title{Linear Algebra for Machine Learning in Python}
\date{\today}
\institute{High Performance Computing and Analytics Lab}
\author{Dr. Moritz Wolter}

\titlegraphic{\includegraphics[width=2.00cm]{UNI_Bonn_Logo_Standard_RZ.pdf}}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\begin{document}
    \maketitle

    \begin{frame}
    \frametitle{Overview} 
    \tableofcontents
    \end{frame}

    \section{Introduction}
    \begin{frame}{Motvating linear algebra}
      TODO
    \end{frame}

    \begin{frame}{Matrices}
      $\mathbf{A} \in \mathbb{R}^{m,n}$ is a real-valued Matrix with $m$ rows and $n$ columns.
      \begin{align}
        \mathbf{A} = \begin{pmatrix}
          a_{11} & a_{12} & \dots & a_{1n} \\
          a_{21} & a_{22} & \dots & a_{2n} \\
          \vdots & \vdots &       & \vdots \\
          a_{m1} & a_{m2} & \dots & a_{mn}
        \end{pmatrix}
      , a_{ij} \in \mathbb{R}.
      \end{align}
     \end{frame}


    \section{Essential operations}
    \begin{frame}{Addition}
      To matrices $\mathbf{A} \in \mathbf{R}^{m,n}$ and $\mathbf{B} \in \mathbf{R}^{m,n}$ can be added by adding their elements.
      \begin{align}
        \mathbf{A} + \mathbf{B} =
        \begin{pmatrix}
            a_{11} + b_{11} & a_{12} + b_{12} & \dots & a_{1n} + b_{1n} \\
            a_{21} + b_{21} & a_{22} + b_{22} & \dots & a_{2n} + b_{2n} \\
            \vdots          & \vdots          & \ddots & \vdots         \\
            a_{m1} + b_{m1} & a_{m2} + b_{m2} & \dots & a_{mn} + b_{mn}
          \end{pmatrix}
        \end{align}
    \end{frame}

    \begin{frame}{Multiplication}
      Multiply $\mathbf{A} \in \mathbb{R}^{m,n}$ by $\mathbf{B} \in \mathbb{R}^{n,p}$ produces $\mathbf{C} \in \mathbb{R}^{m,p}$,
      \begin{align}
      \mathbf{A} \mathbf{B} = \mathbf{C}. 
      \end{align}
      To compute $\mathbf{C}$ the elements in the rows of $\mathbf{A}$ are multiplied with the column elements of $\mathbf{C}$ and the products added,
      \begin{align}
         c_{ik} = \sum_{j=1}^{m} a_{ij} \cdot b_{jk}.
      \end{align}
      \note{
        Define on the board:
        \begin{itemize}
          \item Dot product $\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + \dots + a_n b_n$ for two vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^n$.
          \item Row times column view \cite{strang2009introduction}: 
        \end{itemize}
      }
    \end{frame}

    \begin{frame}{The identity matrix}
      \begin{align}
        \mathbf{I} = \begin{pmatrix}
          1 & & & \\
            &1& & \\
            & & \ddots &\\
            & & &1\\
        \end{pmatrix}
      \end{align}
      \note{Demonstrate multiplication with the inverse by hand.
      TODO
      }
    \end{frame}

    \begin{frame}{Matrix inverse}
      The inverse Matrix $\mathbf{A}^{-1}$ undoes the effects of $\mathbf{A}$, or in mathematical notation,
      \begin{align}
        \mathbf{A}\mathbf{A}^{-1} = \mathbf{I}.
      \end{align}
      The process of computing the inverse is called gaussian elimination.
      \note{
        Example on the board:
        \begin{align}
          \mathbf{A} &= 
          \begin{pmatrix}
            2 & 0 \\
            1 & 3
          \end{pmatrix}
          \rightsquigarrow 
          \begin{pmatrix}[c c | c c]
            2 & 0 & 1 & 0 \\
            1 & 3 & 0 & 1\\ 
          \end{pmatrix}
          \rightsquigarrow
          \begin{pmatrix}[c c | c c]
            1 & 0 & \frac{1}{2} & 0 \\
            1 & 3 & 0 & 1\\ 
          \end{pmatrix} \\
          &\rightsquigarrow
          \begin{pmatrix}[c c | c c]
            1 & 0 & \frac{1}{2} & 0 \\
            0 & 3 & -\frac{1}{2} & 1\\ 
          \end{pmatrix}
          \rightsquigarrow
          \begin{pmatrix}[c c | c c]
            1 & 0 & \frac{1}{2} & 0 \\
            0 & 1 & -\frac{1}{6} & \frac{1}{3}\\ 
          \end{pmatrix}
      \end{align}
      Test the result:
      \begin{align}
          \begin{pmatrix}
            2 & 0 \\
            1 & 3 \\
          \end{pmatrix}
        \begin{pmatrix}
          \frac{1}{2} & 0 \\
          -\frac{1}{6} & \frac{1}{3}\\ 
        \end{pmatrix}
      = 
        \begin{pmatrix}
          2 \cdot \frac{1}{2} + 0 \cdot -\frac{1}{6} & 2 \cdot 0 + 0 \cdot \frac{1}{3}  \\
          1 \cdot \frac{1}{2} + 3 \cdot -\frac{1}{6} & 0 \cdot 0 + 3 \cdot \frac{1}{3} 
        \end{pmatrix}
      =
      \begin{pmatrix}
        1 & 0 \\
        0 & 1 \\ 
      \end{pmatrix}
      \end{align}
      }
    \end{frame}


    \begin{frame}{The Transpose}
      The transpose operation flips matrices along the diagonal, for example in $\mathbb{R}^2$,
      \begin{align}
        \begin{pmatrix}
          a & b \\
          c & d \\
        \end{pmatrix}^T
        =
        \begin{pmatrix}
          a & c \\
          b & d \\
        \end{pmatrix}
      \end{align}
    \end{frame}

    \begin{frame}{Motivation of the determinant}
      TODO
    \end{frame}

    \begin{frame}{Computing determinants in two or three dimensions}
      The two dimensional case:
      \begin{align}
        \begin{vmatrix}
          a_{11} & a_{12} \\
          a_{21} & a_{22} \\
        \end{vmatrix}
        = a_{11} \cdot a_{22} - a_{12} \cdot a_{21} \\
      \end{align}
      Computing the determinant of a three dimensional matrix.
      \begin{align}
        \begin{vmatrix}
          a_{11} & a_{12} & a_{13}  \\
          a_{21} & a_{22} & a_{23}  \\
          a_{31} & a_{32} & a_{33}
        \end{vmatrix}
        = a_{11} \cdot
         \begin{vmatrix}
          a_{21} & a_{23}   \\
          a_{32} & a_{33}   \\
         \end{vmatrix}  
         -
         a_{21} \cdot
         \begin{vmatrix}
          a_{12} & a_{13}   \\
          a_{32} & a_{33}   \\
         \end{vmatrix}  
        +
         a_{31} \cdot
         \begin{vmatrix}
          a_{12} & a_{13}   \\
          a_{22} & a_{23}   \\
         \end{vmatrix}  
      \end{align}

      \note{Draw the sign pattern on the board:
      \begin{align}
        \begin{vmatrix}
          + & - & + & \dots \\
          - & + & - &  \dots \\
          + & - & + &  \dots \\
          \vdots & \vdots & \vdots & \ddots \\
        \end{vmatrix}        
      \end{align}
      The determinant can be expandend along any column as long as the sign pattern is respected.
      }
    \end{frame}

    \begin{frame}{Determinants in n-dimensions}
      \begin{align*}
      \begin{vmatrix}
        a_{11} & a_{21} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots &       & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn}
      \end{vmatrix}
    = a_{11} \begin{vmatrix}
      a_{22} & \dots & a_{2n} \\
      \vdots &        & \vdots \\
      a_{m2} & \dots & a_{mn}
    \end{vmatrix}
    + a_{21} 
    \begin{vmatrix}
      a_{21} & \dots & a_{2n} \\
      \vdots &        & \vdots \\
      a_{m2} & \dots & a_{mn}
    \end{vmatrix}
    \\
    - a_{m1}
    \begin{vmatrix} 
      a_{11} & \dots & a_{1n} \\
      a_{21} & \dots & a_{2n} \\
      \vdots &        & \vdots \\
    \end{vmatrix}
    \end{align*}
    \end{frame}


  \section{Linear curve fitting}
    \begin{frame}{What is the best line connecting measurements?}
    
    \end{frame}

    \begin{frame}{The Pseudoinverse}
        \begin{align}
          \mathbf{A}^{\dagger} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T
        \end{align}
    \note{
      Sometimes solving $\mathbf{A}\mathbf{x} + \mathbf{b} = 0$ is implossible.
      One the board, derive:
      \begin{align}
        \min_x \dfrac{1}{2}|\mathbf{A}\mathbf{x} - \mathbf{b}|^2 \\
      \end{align}
      At the optimum we expect,
      \begin{align}
        0 &= \nabla_x \dfrac{1}{2}|\mathbf{A}\mathbf{x} - \mathbf{b}|^2 \\
          &= \nabla_x \dfrac{1}{2}(\mathbf{A}\mathbf{x} - \mathbf{b})^T(\mathbf{A}\mathbf{x} - \mathbf{b}) \\
          &= (\mathbf{A}\mathbf{x} - \mathbf{b})\mathbf{A}^T \\
          &= \mathbf{A}^T\mathbf{A}\mathbf{x} - \mathbf{A}^T\mathbf{b} \\
          \mathbf{A}^T\mathbf{b} &= \mathbf{A}^T\mathbf{A}\mathbf{x} \\
          (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b} &= \mathbf{x}  
      \end{align}
    }
    \end{frame}

  \section{Regularization}
  \begin{frame}{Eigenvalue-Decomposition}
      TODO
  \end{frame}

  \begin{frame}{Singular-Value-Decomposition}
    TODO
  \end{frame}


  \begin{frame}{Literature}
    \printbibliography
  \end{frame}

\end{document}
